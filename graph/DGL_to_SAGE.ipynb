{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_net_id = 'aks9136'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import dgl \n",
    "from dgl.nn.pytorch.conv import SAGEConv\n",
    "import numpy as np \n",
    "import json \n",
    "import torch\n",
    "from tqdm import tqdm \n",
    "\n",
    "import sys\n",
    "sys.path.append('/scratch/' + user_net_id + '/NYU-Zillow-Capstone-2022-Team-A')\n",
    "import src.datamodules.SAGE as g_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Node Embedding Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading data from cached files.\n"
     ]
    }
   ],
   "source": [
    "####################################\n",
    "## Sample with Correct Formatting ##\n",
    "####################################\n",
    "\n",
    "dataset = dgl.data.CSVDataset('./graph_csv')\n",
    "g = dataset[0]\n",
    "\n",
    "embedding_length = g.ndata['feat'].size()[1]\n",
    "\n",
    "conv = SAGEConv(embedding_length, embedding_length, 'mean')\n",
    "res = conv(g, g.ndata['feat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch 00000 | Loss 0.6846 | Distance Reduced 0.0000 %\n",
      "Epoch 00025 | Loss 0.0418 | Distance Reduced 0.7027 %\n",
      "Epoch 00050 | Loss 0.0247 | Distance Reduced 0.7657 %\n",
      "Epoch 00075 | Loss 0.0218 | Distance Reduced 0.7795 %\n",
      "Epoch 00100 | Loss 0.0208 | Distance Reduced 0.7846 %\n",
      "Epoch 00125 | Loss 0.0202 | Distance Reduced 0.7874 %\n",
      "Epoch 00150 | Loss 0.0199 | Distance Reduced 0.7890 %\n",
      "Epoch 00175 | Loss 0.0198 | Distance Reduced 0.7899 %\n",
      "Epoch 00200 | Loss 0.0197 | Distance Reduced 0.7906 %\n",
      "Epoch 00225 | Loss 0.0196 | Distance Reduced 0.7910 %\n",
      "Epoch 00250 | Loss 0.0195 | Distance Reduced 0.7913 %\n",
      "Epoch 00275 | Loss 0.0195 | Distance Reduced 0.7915 %\n",
      "Epoch 00300 | Loss 0.0195 | Distance Reduced 0.7917 %\n",
      "Epoch 00325 | Loss 0.0194 | Distance Reduced 0.7919 %\n",
      "Epoch 00350 | Loss 0.0194 | Distance Reduced 0.7920 %\n",
      "Epoch 00375 | Loss 0.0194 | Distance Reduced 0.7921 %\n",
      "Epoch 00400 | Loss 0.0194 | Distance Reduced 0.7922 %\n",
      "Epoch 00425 | Loss 0.0194 | Distance Reduced 0.7923 %\n",
      "Epoch 00450 | Loss 0.0194 | Distance Reduced 0.7923 %\n",
      "Epoch 00475 | Loss 0.0194 | Distance Reduced 0.7922 %\n",
      "Testing...\n",
      "Testing Complete\n"
     ]
    }
   ],
   "source": [
    "##################################\n",
    "## Train -> Evaluation Pipeline ##\n",
    "##################################\n",
    "\n",
    "new_node_embeddings = g_train.run(g, 'pool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "## Update Predicted Node Embeddings ##\n",
    "######################################\n",
    "\n",
    "## Adding feature (not overwriting)\n",
    "\n",
    "g.ndata['feat_pred'] = new_node_embeddings\n",
    "dgl.data.utils.save_graphs('graph_csv/coco_val_graph/coco_val_graph.bin', [g])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Link Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "## Data Preperation ##\n",
    "######################\n",
    "\n",
    "test_frac = 0.1\n",
    "\n",
    "## Positive Edges \n",
    "\n",
    "# Split edge set for training and testing\n",
    "u, v = g.edges()\n",
    "\n",
    "eids = np.arange(g.number_of_edges())\n",
    "eids = np.random.permutation(eids)\n",
    "test_size = int(len(eids) * test_frac)\n",
    "train_size = g.number_of_edges() - test_size\n",
    "test_pos_u, test_pos_v = u[eids[:test_size]], v[eids[:test_size]]\n",
    "train_pos_u, train_pos_v = u[eids[test_size:]], v[eids[test_size:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 21324.81it/s]\n"
     ]
    }
   ],
   "source": [
    "## Negative Edges\n",
    "\n",
    "# Find all negative edges and split them for training and testing\n",
    "tag_mask = list(([1] * 80) + ([0] * 5000))\n",
    "tag_loc = torch.ByteTensor(tag_mask)\n",
    "\n",
    "shortened_adj_matrix = g.adjacency_matrix().clone().to_dense()\n",
    "shortened_adj_matrix = torch.transpose(torch.transpose(shortened_adj_matrix,0,1)[tag_loc], 0,1)\n",
    "\n",
    "for i in tqdm(range(shortened_adj_matrix.size()[1], shortened_adj_matrix.size()[0])):\n",
    "    t = shortened_adj_matrix[i]\n",
    "    new_vs = (t == 1.0).nonzero(as_tuple=False).type(torch.IntTensor)\n",
    "    \n",
    "    try:\n",
    "        neg_v_tensor = torch.cat((neg_v_tensor, new_vs)).type(torch.IntTensor)  \n",
    "    except:\n",
    "        neg_v_tensor = new_vs\n",
    "\n",
    "\n",
    "    new_us = [i] * new_vs.size(dim=0)\n",
    "    new_us = torch.Tensor(new_us).type(torch.IntTensor)\n",
    "\n",
    "    try:\n",
    "        neg_u_tensor = torch.cat((neg_u_tensor, new_us)).type(torch.IntTensor)\n",
    "    except:\n",
    "        neg_u_tensor = new_us\n",
    "\n",
    "neg_v_tensor = neg_v_tensor.squeeze()\n",
    "\n",
    "import random\n",
    "negative_indices = list(range(0, len(neg_v_tensor)))\n",
    "random.shuffle(negative_indices)\n",
    "\n",
    "train_indices = negative_indices[test_size:]\n",
    "test_indices = negative_indices[:test_size]\n",
    "\n",
    "test_neg_u = neg_u_tensor[test_indices]\n",
    "test_neg_v = neg_v_tensor[test_indices]\n",
    "\n",
    "train_neg_u = neg_u_tensor[train_indices]\n",
    "train_neg_v = neg_v_tensor[train_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "## Create Training Graph ##\n",
    "###########################\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "train_pos_g = dgl.graph((train_pos_u, train_pos_v)).to(device)\n",
    "train_neg_g = dgl.graph((train_neg_u, train_neg_v)).to(device)\n",
    "\n",
    "test_pos_g = dgl.graph((test_pos_u, test_pos_v)).to(device)\n",
    "test_neg_g = dgl.graph((test_neg_u, test_neg_v)).to(device)\n",
    "\n",
    "train_g = dgl.remove_edges(g, eids[:test_size])\n",
    "\n",
    "remove_ind = np.random.choice(train_g.num_nodes(), train_g.num_nodes() -  test_pos_g.num_nodes())\n",
    "train_g.remove_nodes(remove_ind)\n",
    "\n",
    "remove_ind = np.random.choice(train_pos_g.num_nodes(), train_pos_g.num_nodes() -  test_pos_g.num_nodes())\n",
    "train_pos_g.remove_nodes(remove_ind)\n",
    "\n",
    "remove_ind = np.random.choice(train_neg_g.num_nodes(), train_neg_g.num_nodes() -  test_pos_g.num_nodes())\n",
    "train_neg_g.remove_nodes(remove_ind)\n",
    "\n",
    "remove_ind = np.random.choice(test_neg_g.num_nodes(), test_neg_g.num_nodes() -  test_pos_g.num_nodes())\n",
    "test_neg_g.remove_nodes(remove_ind)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "## Predicton Function ##\n",
    "########################\n",
    "\n",
    "import dgl.function as fn\n",
    "\n",
    "class DotPredictor(torch.nn.Module):\n",
    "    def forward(self, g, h):\n",
    "        with g.local_scope():\n",
    "            g = g.to(device)\n",
    "            g.ndata['h'] = h.to(device)\n",
    "            # Compute a new edge feature named 'score' by a dot-product between the\n",
    "            # source node feature 'h' and destination node feature 'h'.\n",
    "            g.apply_edges(fn.u_dot_v('h', 'h', 'score'))\n",
    "            # u_dot_v returns a 1-element vector for each edge so you need to squeeze it.\n",
    "            return g.edata['score'][:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "## Training Loop - Setup ##\n",
    "###########################\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "model = g_train.SAGE(train_g.ndata['feat'].shape[1], None ,train_g.ndata['feat'].shape[1], 'mean')\n",
    "# You can replace DotPredictor with MLPPredictor.\n",
    "#pred = MLPPredictor(16)\n",
    "pred = DotPredictor()\n",
    "\n",
    "## Note: loss can be greater than one because labels are 1s and 0s \n",
    "def compute_loss(pos_score, neg_score):\n",
    "    scores = torch.cat([pos_score, neg_score])\n",
    "    labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])])\n",
    "    return F.binary_cross_entropy_with_logits(scores, labels)\n",
    "\n",
    "def compute_auc(pos_score, neg_score):\n",
    "    scores = torch.cat([pos_score, neg_score]).numpy()\n",
    "    labels = torch.cat(\n",
    "        [torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]).numpy()\n",
    "    return roc_auc_score(labels, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, loss: 22.82440185546875\n",
      "In epoch 100, loss: 0.481178343296051\n",
      "In epoch 200, loss: 0.28091341257095337\n"
     ]
    }
   ],
   "source": [
    "#############################\n",
    "## Training Loop Execution ##\n",
    "#############################\n",
    "\n",
    "import itertools\n",
    "\n",
    "optimizer = torch.optim.Adam(itertools.chain(model.parameters(), pred.parameters()), lr=0.05)\n",
    "\n",
    "for e in range(300):\n",
    "    # forward\n",
    "    h = model(train_g, train_g.ndata['feat']).to(device)\n",
    "    pos_score = pred(train_pos_g, h)\n",
    "    neg_score = pred(train_neg_g, h)\n",
    "    loss = compute_loss(pos_score, neg_score)\n",
    "\n",
    "    # backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if e % 100 == 0:\n",
    "        print('In epoch {}, loss: {}'.format(e, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC 0.5406468827521459\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    pos_score = pred(test_pos_g.to(device), h.to(device))\n",
    "    neg_score = pred(test_neg_g.to(device), h.to(device))\n",
    "    print('AUC', compute_auc(pos_score, neg_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b8a98180768ec50653acfbae9679ecd2014a1d8366e4dd2cee9bea05835201d8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
