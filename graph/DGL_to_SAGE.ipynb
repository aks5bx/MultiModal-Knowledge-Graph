{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/conda/zillow_MMKG/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import dgl \n",
    "from dgl.nn.pytorch.conv import SAGEConv\n",
    "import numpy as np \n",
    "import json \n",
    "import torch\n",
    "\n",
    "import graph_training as g_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading data from cached files.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4729, -0.4447, -0.2980,  ...,  0.3795,  0.3539, -0.9945],\n",
       "        [-0.3101, -0.5475, -0.4219,  ...,  0.4979, -0.1411, -0.8216],\n",
       "        [-0.3417, -0.2552, -0.1960,  ...,  0.5045,  0.1464, -1.2719],\n",
       "        ...,\n",
       "        [-0.2411, -0.5378, -0.1492,  ..., -0.6615, -0.2417, -0.2994],\n",
       "        [-0.2453, -0.3503, -0.9449,  ...,  0.0665, -0.3339, -0.0525],\n",
       "        [ 0.0911, -0.1186, -0.9535,  ...,  0.1819, -0.8704, -0.2218]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####################################\n",
    "## Sample with Correct Formatting ##\n",
    "####################################\n",
    "\n",
    "dataset = dgl.data.CSVDataset('./graph_csv')\n",
    "g = dataset[0]\n",
    "\n",
    "embedding_length = g.ndata['feat'].size()[1]\n",
    "\n",
    "conv = SAGEConv(embedding_length, embedding_length, 'mean')\n",
    "res = conv(g, g.ndata['feat'])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch 00000 | Loss 0.7841 | Accuracy 585.6627 \n",
      "Epoch 00001 | Loss 0.5269 | Accuracy 796.2719 \n",
      "Epoch 00002 | Loss 0.3362 | Accuracy 1035.8636 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/aks9136/NYU-Zillow-Capstone-2022-Team-A/graph/graph_training.py:53: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/IndexingUtils.h:27.)\n",
      "  loss = loss_fcn(logits[train_mask], g.ndata['feat'][train_mask])\n",
      "/ext3/conda/zillow_MMKG/lib/python3.8/site-packages/torch/autograd/__init__.py:191: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/IndexingUtils.h:27.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "/scratch/aks9136/NYU-Zillow-Capstone-2022-Team-A/graph/graph_training.py:36: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/IndexingUtils.h:27.)\n",
      "  preds = preds[mask]\n",
      "/scratch/aks9136/NYU-Zillow-Capstone-2022-Team-A/graph/graph_training.py:37: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/IndexingUtils.h:27.)\n",
      "  labels = labels[mask]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00003 | Loss 0.2021 | Accuracy 1266.1562 \n",
      "Epoch 00004 | Loss 0.1041 | Accuracy 1474.4838 \n",
      "Epoch 00005 | Loss 0.0320 | Accuracy 1657.6792 \n",
      "Epoch 00006 | Loss -0.0268 | Accuracy 1816.0425 \n",
      "Epoch 00007 | Loss -0.0732 | Accuracy 1951.6732 \n",
      "Epoch 00008 | Loss -0.1140 | Accuracy 2067.0828 \n",
      "Epoch 00009 | Loss -0.1494 | Accuracy 2164.9224 \n",
      "Epoch 00010 | Loss -0.1808 | Accuracy 2247.7510 \n",
      "Epoch 00011 | Loss -0.2153 | Accuracy 2317.6465 \n",
      "Epoch 00012 | Loss -0.2405 | Accuracy 2376.7378 \n",
      "Epoch 00013 | Loss -0.2658 | Accuracy 2426.8801 \n",
      "Epoch 00014 | Loss -0.2889 | Accuracy 2469.7158 \n",
      "Epoch 00015 | Loss -0.3123 | Accuracy 2506.6279 \n",
      "Epoch 00016 | Loss -0.3352 | Accuracy 2538.7554 \n",
      "Epoch 00017 | Loss -0.3537 | Accuracy 2567.2026 \n",
      "Epoch 00018 | Loss -0.3684 | Accuracy 2592.9312 \n",
      "Epoch 00019 | Loss -0.3857 | Accuracy 2616.7214 \n",
      "Epoch 00020 | Loss -0.4049 | Accuracy 2639.0396 \n",
      "Epoch 00021 | Loss -0.4211 | Accuracy 2660.3613 \n",
      "Epoch 00022 | Loss -0.4366 | Accuracy 2681.1191 \n",
      "Epoch 00023 | Loss -0.4510 | Accuracy 2701.6116 \n",
      "Epoch 00024 | Loss -0.4614 | Accuracy 2722.1345 \n",
      "Epoch 00025 | Loss -0.4774 | Accuracy 2742.8767 \n",
      "Epoch 00026 | Loss -0.4916 | Accuracy 2763.9651 \n",
      "Epoch 00027 | Loss -0.4965 | Accuracy 2785.5754 \n",
      "Epoch 00028 | Loss -0.5153 | Accuracy 2807.6917 \n",
      "Epoch 00029 | Loss -0.5273 | Accuracy 2830.2234 \n",
      "Epoch 00030 | Loss -0.5345 | Accuracy 2853.4114 \n",
      "Epoch 00031 | Loss -0.5496 | Accuracy 2876.9839 \n",
      "Epoch 00032 | Loss -0.5625 | Accuracy 2901.1184 \n",
      "Epoch 00033 | Loss -0.5705 | Accuracy 2925.7822 \n",
      "Epoch 00034 | Loss -0.5868 | Accuracy 2950.5339 \n",
      "Epoch 00035 | Loss -0.5967 | Accuracy 2975.5776 \n",
      "Epoch 00036 | Loss -0.6067 | Accuracy 3000.9192 \n",
      "Epoch 00037 | Loss -0.6133 | Accuracy 3026.6372 \n",
      "Epoch 00038 | Loss -0.6305 | Accuracy 3052.4529 \n",
      "Epoch 00039 | Loss -0.6364 | Accuracy 3078.4636 \n",
      "Epoch 00040 | Loss -0.6562 | Accuracy 3104.2766 \n",
      "Epoch 00041 | Loss -0.6650 | Accuracy 3130.0881 \n",
      "Epoch 00042 | Loss -0.6759 | Accuracy 3155.8445 \n",
      "Epoch 00043 | Loss -0.6861 | Accuracy 3181.6162 \n",
      "Epoch 00044 | Loss -0.7007 | Accuracy 3207.3411 \n",
      "Epoch 00045 | Loss -0.7142 | Accuracy 3233.1018 \n",
      "Epoch 00046 | Loss -0.7209 | Accuracy 3258.9292 \n",
      "Epoch 00047 | Loss -0.7345 | Accuracy 3284.6328 \n",
      "Epoch 00048 | Loss -0.7380 | Accuracy 3310.4568 \n",
      "Epoch 00049 | Loss -0.7547 | Accuracy 3336.3228 \n",
      "Epoch 00050 | Loss -0.7634 | Accuracy 3362.1855 \n",
      "Epoch 00051 | Loss -0.7719 | Accuracy 3388.0967 \n",
      "Epoch 00052 | Loss -0.7848 | Accuracy 3413.8838 \n",
      "Epoch 00053 | Loss -0.7981 | Accuracy 3439.4131 \n",
      "Epoch 00054 | Loss -0.8076 | Accuracy 3464.7288 \n",
      "Epoch 00055 | Loss -0.8253 | Accuracy 3489.7808 \n",
      "Epoch 00056 | Loss -0.8315 | Accuracy 3514.7048 \n",
      "Epoch 00057 | Loss -0.8395 | Accuracy 3539.5161 \n",
      "Epoch 00058 | Loss -0.8456 | Accuracy 3564.3137 \n",
      "Epoch 00059 | Loss -0.8614 | Accuracy 3588.9084 \n",
      "Epoch 00060 | Loss -0.8680 | Accuracy 3613.3127 \n",
      "Epoch 00061 | Loss -0.8807 | Accuracy 3637.5293 \n",
      "Epoch 00062 | Loss -0.8973 | Accuracy 3661.5908 \n",
      "Epoch 00063 | Loss -0.9081 | Accuracy 3685.5168 \n",
      "Epoch 00064 | Loss -0.9086 | Accuracy 3709.3906 \n",
      "Epoch 00065 | Loss -0.9248 | Accuracy 3733.2615 \n",
      "Epoch 00066 | Loss -0.9302 | Accuracy 3757.2019 \n",
      "Epoch 00067 | Loss -0.9444 | Accuracy 3781.0068 \n",
      "Epoch 00068 | Loss -0.9473 | Accuracy 3804.8157 \n",
      "Epoch 00069 | Loss -0.9650 | Accuracy 3828.5742 \n",
      "Epoch 00070 | Loss -0.9689 | Accuracy 3852.2139 \n",
      "Epoch 00071 | Loss -0.9838 | Accuracy 3875.7300 \n",
      "Epoch 00072 | Loss -0.9924 | Accuracy 3899.0674 \n",
      "Epoch 00073 | Loss -0.9983 | Accuracy 3922.4468 \n",
      "Epoch 00074 | Loss -1.0101 | Accuracy 3945.8293 \n",
      "Epoch 00075 | Loss -1.0220 | Accuracy 3969.2131 \n",
      "Epoch 00076 | Loss -1.0269 | Accuracy 3992.4414 \n",
      "Epoch 00077 | Loss -1.0408 | Accuracy 4015.5498 \n",
      "Epoch 00078 | Loss -1.0571 | Accuracy 4038.3171 \n",
      "Epoch 00079 | Loss -1.0561 | Accuracy 4060.9365 \n",
      "Epoch 00080 | Loss -1.0679 | Accuracy 4083.5525 \n",
      "Epoch 00081 | Loss -1.0778 | Accuracy 4106.1245 \n",
      "Epoch 00082 | Loss -1.0905 | Accuracy 4128.6738 \n",
      "Epoch 00083 | Loss -1.1042 | Accuracy 4151.1387 \n",
      "Epoch 00084 | Loss -1.1031 | Accuracy 4173.7212 \n",
      "Epoch 00085 | Loss -1.1212 | Accuracy 4196.1328 \n",
      "Epoch 00086 | Loss -1.1207 | Accuracy 4218.6133 \n",
      "Epoch 00087 | Loss -1.1360 | Accuracy 4240.8799 \n",
      "Epoch 00088 | Loss -1.1372 | Accuracy 4263.0430 \n",
      "Epoch 00089 | Loss -1.1632 | Accuracy 4285.0015 \n",
      "Epoch 00090 | Loss -1.1658 | Accuracy 4306.9282 \n",
      "Epoch 00091 | Loss -1.1794 | Accuracy 4328.7090 \n",
      "Epoch 00092 | Loss -1.1738 | Accuracy 4350.4326 \n",
      "Epoch 00093 | Loss -1.1882 | Accuracy 4372.0947 \n",
      "Epoch 00094 | Loss -1.2087 | Accuracy 4393.6323 \n",
      "Epoch 00095 | Loss -1.2036 | Accuracy 4415.3057 \n",
      "Epoch 00096 | Loss -1.2269 | Accuracy 4436.8750 \n",
      "Epoch 00097 | Loss -1.2274 | Accuracy 4458.3652 \n",
      "Epoch 00098 | Loss -1.2455 | Accuracy 4479.6392 \n",
      "Epoch 00099 | Loss -1.2434 | Accuracy 4501.0874 \n",
      "Epoch 00100 | Loss -1.2556 | Accuracy 4522.4146 \n",
      "Epoch 00101 | Loss -1.2646 | Accuracy 4543.7046 \n",
      "Epoch 00102 | Loss -1.2712 | Accuracy 4564.9180 \n",
      "Epoch 00103 | Loss -1.2857 | Accuracy 4585.9092 \n",
      "Epoch 00104 | Loss -1.2901 | Accuracy 4606.8599 \n",
      "Epoch 00105 | Loss -1.2973 | Accuracy 4627.8301 \n",
      "Epoch 00106 | Loss -1.3138 | Accuracy 4648.8125 \n",
      "Epoch 00107 | Loss -1.3122 | Accuracy 4669.9248 \n",
      "Epoch 00108 | Loss -1.3260 | Accuracy 4691.0156 \n",
      "Epoch 00109 | Loss -1.3322 | Accuracy 4712.1694 \n",
      "Epoch 00110 | Loss -1.3514 | Accuracy 4733.0669 \n",
      "Epoch 00111 | Loss -1.3417 | Accuracy 4753.9214 \n",
      "Epoch 00112 | Loss -1.3567 | Accuracy 4774.6250 \n",
      "Epoch 00113 | Loss -1.3613 | Accuracy 4795.2876 \n",
      "Epoch 00114 | Loss -1.3758 | Accuracy 4815.8760 \n",
      "Epoch 00115 | Loss -1.3956 | Accuracy 4836.1523 \n",
      "Epoch 00116 | Loss -1.3952 | Accuracy 4856.1787 \n",
      "Epoch 00117 | Loss -1.4026 | Accuracy 4876.2334 \n",
      "Epoch 00118 | Loss -1.3965 | Accuracy 4896.3647 \n",
      "Epoch 00119 | Loss -1.4134 | Accuracy 4916.4517 \n",
      "Epoch 00120 | Loss -1.4194 | Accuracy 4936.5498 \n",
      "Epoch 00121 | Loss -1.4285 | Accuracy 4956.5889 \n",
      "Epoch 00122 | Loss -1.4386 | Accuracy 4976.6372 \n",
      "Epoch 00123 | Loss -1.4575 | Accuracy 4996.5610 \n",
      "Epoch 00124 | Loss -1.4631 | Accuracy 5016.4580 \n",
      "Epoch 00125 | Loss -1.4665 | Accuracy 5036.2476 \n",
      "Epoch 00126 | Loss -1.4752 | Accuracy 5055.9014 \n",
      "Epoch 00127 | Loss -1.4825 | Accuracy 5075.5884 \n",
      "Epoch 00128 | Loss -1.4928 | Accuracy 5095.1655 \n",
      "Epoch 00129 | Loss -1.5055 | Accuracy 5114.8071 \n",
      "Epoch 00130 | Loss -1.5108 | Accuracy 5134.2393 \n",
      "Epoch 00131 | Loss -1.5213 | Accuracy 5153.5376 \n",
      "Epoch 00132 | Loss -1.5124 | Accuracy 5172.8555 \n",
      "Epoch 00133 | Loss -1.5383 | Accuracy 5192.0493 \n",
      "Epoch 00134 | Loss -1.5481 | Accuracy 5211.1582 \n",
      "Epoch 00135 | Loss -1.5392 | Accuracy 5230.3540 \n",
      "Epoch 00136 | Loss -1.5577 | Accuracy 5249.4800 \n",
      "Epoch 00137 | Loss -1.5591 | Accuracy 5268.8223 \n",
      "Epoch 00138 | Loss -1.5684 | Accuracy 5288.1709 \n",
      "Epoch 00139 | Loss -1.5872 | Accuracy 5307.2690 \n",
      "Epoch 00140 | Loss -1.5770 | Accuracy 5326.3408 \n",
      "Epoch 00141 | Loss -1.6001 | Accuracy 5345.2432 \n",
      "Epoch 00142 | Loss -1.5973 | Accuracy 5364.2070 \n",
      "Epoch 00143 | Loss -1.6193 | Accuracy 5382.9248 \n",
      "Epoch 00144 | Loss -1.6263 | Accuracy 5401.5107 \n",
      "Epoch 00145 | Loss -1.6352 | Accuracy 5419.9585 \n",
      "Epoch 00146 | Loss -1.6408 | Accuracy 5438.2578 \n",
      "Epoch 00147 | Loss -1.6589 | Accuracy 5456.5684 \n",
      "Epoch 00148 | Loss -1.6533 | Accuracy 5474.8208 \n",
      "Epoch 00149 | Loss -1.6632 | Accuracy 5493.3394 \n",
      "Epoch 00150 | Loss -1.6690 | Accuracy 5512.0317 \n",
      "Epoch 00151 | Loss -1.6756 | Accuracy 5530.7534 \n",
      "Epoch 00152 | Loss -1.6874 | Accuracy 5549.3545 \n",
      "Epoch 00153 | Loss -1.6903 | Accuracy 5568.0537 \n",
      "Epoch 00154 | Loss -1.6904 | Accuracy 5586.9160 \n",
      "Epoch 00155 | Loss -1.7147 | Accuracy 5605.7861 \n",
      "Epoch 00156 | Loss -1.7124 | Accuracy 5624.7227 \n",
      "Epoch 00157 | Loss -1.7228 | Accuracy 5643.3804 \n",
      "Epoch 00158 | Loss -1.7205 | Accuracy 5661.9458 \n",
      "Epoch 00159 | Loss -1.7386 | Accuracy 5680.3491 \n",
      "Epoch 00160 | Loss -1.7343 | Accuracy 5698.7178 \n",
      "Epoch 00161 | Loss -1.7500 | Accuracy 5716.9595 \n",
      "Epoch 00162 | Loss -1.7502 | Accuracy 5735.1284 \n",
      "Epoch 00163 | Loss -1.7869 | Accuracy 5753.1240 \n",
      "Epoch 00164 | Loss -1.7679 | Accuracy 5771.1079 \n",
      "Epoch 00165 | Loss -1.7862 | Accuracy 5789.0596 \n",
      "Epoch 00166 | Loss -1.7952 | Accuracy 5807.0020 \n",
      "Epoch 00167 | Loss -1.7980 | Accuracy 5824.9961 \n",
      "Epoch 00168 | Loss -1.7914 | Accuracy 5843.2109 \n",
      "Epoch 00169 | Loss -1.8125 | Accuracy 5861.3496 \n",
      "Epoch 00170 | Loss -1.8035 | Accuracy 5879.4805 \n",
      "Epoch 00171 | Loss -1.8295 | Accuracy 5897.5107 \n",
      "Epoch 00172 | Loss -1.8412 | Accuracy 5915.5464 \n",
      "Epoch 00173 | Loss -1.8599 | Accuracy 5933.3223 \n",
      "Epoch 00174 | Loss -1.8345 | Accuracy 5951.1460 \n",
      "Epoch 00175 | Loss -1.8541 | Accuracy 5968.9492 \n",
      "Epoch 00176 | Loss -1.8687 | Accuracy 5986.6787 \n",
      "Epoch 00177 | Loss -1.8622 | Accuracy 6004.4658 \n",
      "Epoch 00178 | Loss -1.8678 | Accuracy 6022.2446 \n",
      "Epoch 00179 | Loss -1.8949 | Accuracy 6039.8657 \n",
      "Epoch 00180 | Loss -1.8826 | Accuracy 6057.4121 \n",
      "Epoch 00181 | Loss -1.9115 | Accuracy 6074.8257 \n",
      "Epoch 00182 | Loss -1.9145 | Accuracy 6092.0415 \n",
      "Epoch 00183 | Loss -1.9021 | Accuracy 6109.1772 \n",
      "Epoch 00184 | Loss -1.9227 | Accuracy 6126.5459 \n",
      "Epoch 00185 | Loss -1.9436 | Accuracy 6143.8359 \n",
      "Epoch 00186 | Loss -1.9395 | Accuracy 6161.3125 \n",
      "Epoch 00187 | Loss -1.9444 | Accuracy 6178.9272 \n",
      "Epoch 00188 | Loss -1.9613 | Accuracy 6196.3213 \n",
      "Epoch 00189 | Loss -1.9731 | Accuracy 6213.7573 \n",
      "Epoch 00190 | Loss -1.9686 | Accuracy 6231.0317 \n",
      "Epoch 00191 | Loss -1.9521 | Accuracy 6248.3760 \n",
      "Epoch 00192 | Loss -1.9591 | Accuracy 6265.7329 \n",
      "Epoch 00193 | Loss -1.9792 | Accuracy 6283.0044 \n",
      "Epoch 00194 | Loss -1.9949 | Accuracy 6300.2109 \n",
      "Epoch 00195 | Loss -1.9869 | Accuracy 6317.1733 \n",
      "Epoch 00196 | Loss -2.0076 | Accuracy 6333.9468 \n",
      "Epoch 00197 | Loss -2.0132 | Accuracy 6350.5303 \n",
      "Epoch 00198 | Loss -2.0311 | Accuracy 6366.8506 \n",
      "Epoch 00199 | Loss -2.0253 | Accuracy 6383.1523 \n",
      "Testing...\n",
      "Test accuracy 6318.7061\n"
     ]
    }
   ],
   "source": [
    "##################################\n",
    "## Train -> Evaluation Pipeline ##\n",
    "##################################\n",
    "\n",
    "g_train.run(g, 'gcn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b8a98180768ec50653acfbae9679ecd2014a1d8366e4dd2cee9bea05835201d8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
