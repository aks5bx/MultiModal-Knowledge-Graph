{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from functools import partial\n",
    "\n",
    "# Simulate having cfg available by loading in hydra config as dict\n",
    "import yaml\n",
    "try:\n",
    "    from yaml import CLoader as Loader, CDumper as Dumper\n",
    "except ImportError:\n",
    "    from yaml import Loader, Dumper\n",
    "\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "import hydra\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics import MeanMetric\n",
    "from pytorch_lightning import LightningDataModule, LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "user_net_id = os.getlogin()\n",
    "home_path = '/scratch/' + user_net_id + '/projects/NYU-Zillow-Capstone-2022-Team-A'\n",
    "if home_path not in sys.path:\n",
    "    sys.path.append('/scratch/' + user_net_id + '/projects/NYU-Zillow-Capstone-2022-Team-A')\n",
    "\n",
    "from src.datamodules.negative_sampler import NegativeSampler\n",
    "from src.model.SAGE import SAGE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate having cfg available by loading in hydra config as dict\n",
    "from types import SimpleNamespace\n",
    "\n",
    "class NestedNamespace(SimpleNamespace):\n",
    "    def __init__(self, dictionary, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        for key, value in dictionary.items():\n",
    "            if isinstance(value, dict):\n",
    "                self.__setattr__(key, NestedNamespace(value))\n",
    "            else:\n",
    "                self.__setattr__(key, value)\n",
    "\n",
    "cfg = NestedNamespace(yaml.load(open('../conf/config.yaml'), Loader=Loader))\n",
    "\n",
    "import json\n",
    "csv_dataset_root = '../' + cfg.data.zillow_root\n",
    "modal_node_ids = json.load(open(f'{csv_dataset_root}/modal_node_ids.json', 'r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTEBOOK ORGANIZATION:\n",
    "This notebook is for exploring and understanding train_graphsage.py. Given that main() runs train(cfg), where cfg is passed from Hydra, we can explore the notebook by deep diving into each function, class, and procedure that goes into train(). The train() function is provided below, and subsequent cells investigate intermediate outputs of its function calls and other important code blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Exploring DataModule Class\n",
    "\n",
    "Below we show the DataModule class definition and dependencies, then execute lines of DataModule, step by step, and examine outputs to understand intermediate values and what's going on overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_bidirected_with_reverse_mapping(g):\n",
    "    \"\"\"Makes a graph bidirectional, and returns a mapping array ``mapping`` where ``mapping[i]``\n",
    "    is the reverse edge of edge ID ``i``. Does not work with graphs that have self-loops.\n",
    "    \"\"\"\n",
    "    g_simple, mapping = dgl.to_simple(\n",
    "        dgl.add_reverse_edges(g), return_counts=\"count\", writeback_mapping=True\n",
    "    )\n",
    "    c = g_simple.edata[\"count\"]\n",
    "    num_edges = g.num_edges()\n",
    "    mapping_offset = torch.zeros(g_simple.num_edges() + 1, dtype=g_simple.idtype)\n",
    "    mapping_offset[1:] = c.cumsum(0)\n",
    "    idx = mapping.argsort()\n",
    "    idx_uniq = idx[mapping_offset[:-1]]\n",
    "    reverse_idx = torch.where(\n",
    "        idx_uniq >= num_edges, idx_uniq - num_edges, idx_uniq + num_edges\n",
    "    )\n",
    "    reverse_mapping = mapping[reverse_idx]\n",
    "    # sanity check\n",
    "    src1, dst1 = g_simple.edges()\n",
    "    src2, dst2 = g_simple.find_edges(reverse_mapping)\n",
    "    assert torch.equal(src1, dst2)\n",
    "    assert torch.equal(src2, dst1)\n",
    "    return g_simple, reverse_mapping\n",
    "\n",
    "# def exclude_edges(eids, reverse_eids, g, max_img_id, device):\n",
    "#     src, dst = g.find_edges(eids)\n",
    "#     eids_exclude = []\n",
    "#     for i in range(len(eids)):\n",
    "#         if (src[i]>max_img_id):\n",
    "#             eids_exclude.append(eids[i])\n",
    "#     eids_exclude = torch.cat((reverse_eids, torch.Tensor(eids_exclude).type(torch.LongTensor).to(device)))\n",
    "    \n",
    "#     return(eids_exclude)\n",
    "\n",
    "class NegativeSamplerTest(object):\n",
    "    def __init__(self, g, k, max_img_id, neg_share=False):\n",
    "        self.weights = g.in_degrees().float() ** 0.75\n",
    "        self.k = k\n",
    "        self.neg_share = neg_share\n",
    "        self.max_img_id = max_img_id\n",
    "\n",
    "    def __call__(self, g, eids):\n",
    "        src, _ = g.find_edges(eids)\n",
    "        img_node_mask = src <= self.max_img_id\n",
    "        src = src[img_node_mask]\n",
    "        n = len(src)\n",
    "\n",
    "        if self.neg_share and n % self.k == 0:\n",
    "            dst = self.weights.multinomial(n, replacement=True)\n",
    "            dst = dst.view(-1, 1, self.k).expand(-1, self.k, -1).flatten()\n",
    "        else:\n",
    "            dst = self.weights.multinomial(n * self.k, replacement=True)\n",
    "            \n",
    "        src = src.repeat_interleave(self.k)\n",
    "        return src, dst\n",
    "\n",
    "class DataModule(LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        csv_dataset_root,\n",
    "        modal_node_ids_file,\n",
    "        data_cpu=False,\n",
    "        fan_out=[10, 25],\n",
    "        device=\"cpu\",\n",
    "        batch_size=1024,\n",
    "        num_workers=4,\n",
    "        force_reload=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        dataset = dgl.data.CSVDataset(csv_dataset_root, force_reload=force_reload)\n",
    "        g = dataset[0]\n",
    "        g_bid, reverse_eids = to_bidirected_with_reverse_mapping(g)\n",
    "        # g = g.formats([\"csc\"])\n",
    "        g_bid = g_bid.to(device)\n",
    "        g = g.to(device)\n",
    "        reverse_eids = reverse_eids.to(device)\n",
    "        # seed_edges = torch.arange(g.num_edges()).to(device)\n",
    "\n",
    "        max_img_id = max(json.load(open(modal_node_ids_file, 'r'))['images'])\n",
    "\n",
    "        train_nid = torch.nonzero(g_bid.ndata[\"train_mask\"], as_tuple=True)[0].to(device)\n",
    "        val_nid = torch.nonzero(g_bid.ndata[\"val_mask\"], as_tuple=True)[0].to(device)\n",
    "        test_nid = torch.nonzero(\n",
    "            ~(g_bid.ndata[\"train_mask\"] | g_bid.ndata[\"val_mask\"]), as_tuple=True\n",
    "        )[0].to(device)\n",
    "\n",
    "        sampler = dgl.dataloading.MultiLayerNeighborSampler(\n",
    "            [int(_) for _ in fan_out], prefetch_node_feats=[\"feat\"]\n",
    "        )\n",
    "\n",
    "        self.g = g\n",
    "        self.g_bid = g_bid\n",
    "        self.train_nid, self.val_nid, self.test_nid = train_nid, val_nid, test_nid\n",
    "        self.sampler = sampler\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.in_dim = g_bid.ndata[\"feat\"].shape[1]\n",
    "        self.reverse_eids = reverse_eids\n",
    "        self.max_img_id = max_img_id\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        #exclude_edges_func = partial(exclude_edges, reverse_eids=self.reverse_eids, g=self.g_bid, max_img_id=self.max_img_id, device=self.device)\n",
    "        \n",
    "        sampler = dgl.dataloading.as_edge_prediction_sampler(\n",
    "            self.sampler,\n",
    "            #exclude=exclude_edges_func,\n",
    "            exclude='reverse_id',\n",
    "            reverse_eids=self.reverse_eids,\n",
    "            negative_sampler=NegativeSamplerTest(self.g, 5, self.max_img_id)\n",
    "            # negative_sampler=dgl.dataloading.negative_sampler.PerSourceUniform(5),\n",
    "        )\n",
    "\n",
    "        return dgl.dataloading.DataLoader(\n",
    "            self.g_bid,\n",
    "            self.train_nid,\n",
    "            sampler,\n",
    "            device=self.device,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=False,\n",
    "            # num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        #exclude_edges_func = partial(exclude_edges, reverse_eids=self.reverse_eids, g=self.g_bid, max_img_id=self.max_img_id, device=self.device)\n",
    "        \n",
    "        sampler = dgl.dataloading.as_edge_prediction_sampler(\n",
    "            self.sampler,\n",
    "            #exclude=exclude_edges_func,\n",
    "            exclude='reverse_id',\n",
    "            reverse_eids=self.reverse_eids,\n",
    "            negative_sampler=NegativeSamplerTest(self.g, 5, self.max_img_id)\n",
    "            # negative_sampler=dgl.dataloading.negative_sampler.PerSourceUniform(5),\n",
    "        )\n",
    "\n",
    "        return dgl.dataloading.DataLoader(\n",
    "            self.g_bid,\n",
    "            self.val_nid,\n",
    "            sampler,\n",
    "            device=self.device,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=False,\n",
    "            # num_workers=self.num_workers,\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        sampler = dgl.dataloading.as_edge_prediction_sampler(\n",
    "            self.sampler,\n",
    "            #exclude=exclude_edges_func,\n",
    "            exclude='reverse_id',\n",
    "            reverse_eids=self.reverse_eids,\n",
    "            negative_sampler=NegativeSamplerTest(self.g, 5, self.max_img_id)\n",
    "            # negative_sampler=dgl.dataloading.negative_sampler.PerSourceUniform(5),\n",
    "        )\n",
    "\n",
    "        return dgl.dataloading.DataLoader(\n",
    "            self.g_bid,\n",
    "            self.test_nid,\n",
    "            sampler,\n",
    "            device=self.device,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=False,\n",
    "            # num_workers=self.num_workers,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading data from cached files.\n"
     ]
    }
   ],
   "source": [
    "# Exploring DataModule class\n",
    "csv_dataset_root = '../' + cfg.data.zillow_root\n",
    "force_reload=False\n",
    "dataset = dgl.data.CSVDataset(csv_dataset_root, force_reload=force_reload)\n",
    "g = dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Exploring SAGELightning Class\n",
    "\n",
    "This is our model that we will train on graph data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScorePredictor(nn.Module):\n",
    "    def forward(self, edge_subgraph, x):\n",
    "        with edge_subgraph.local_scope():\n",
    "            edge_subgraph.ndata[\"h\"] = x\n",
    "            edge_subgraph.apply_edges(fn.u_dot_v(\"h\", \"h\", \"score\"))\n",
    "            return edge_subgraph.edata[\"score\"]\n",
    "\n",
    "\n",
    "class SAGELightning(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim,\n",
    "        h_dim,\n",
    "        n_layers=3,\n",
    "        activation=F.relu,\n",
    "        dropout=0,\n",
    "        sage_conv_method=\"mean\",\n",
    "        lr=0.0005,\n",
    "        batch_size=1024,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.module = SAGE(\n",
    "            in_dim, h_dim, n_layers, activation, dropout, sage_conv_method\n",
    "        )\n",
    "        self.lr = lr\n",
    "        self.predictor = ScorePredictor()\n",
    "        self.batch_size = batch_size\n",
    "        self.save_hyperparameters() # specify that the module should save hyperparameters on each training loop\n",
    "\n",
    "        self.train_loss = MeanMetric() # aggregate loss per batch using mean\n",
    "        self.val_positive_distance = MeanMetric()\n",
    "        self.val_negative_distance = MeanMetric()\n",
    "\n",
    "    def forward(self, graph, blocks, x):\n",
    "        self.module(graph, blocks, x) # Just runs SAGE() model defined in SAGE.py\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_nodes, pos_graph, neg_graph, blocks = batch\n",
    "        x = blocks[0].srcdata[\"feat\"]\n",
    "        logits = self.module(blocks, x)\n",
    "        pos_score = self.predictor(pos_graph, logits)\n",
    "        neg_score = self.predictor(neg_graph, logits)\n",
    "\n",
    "        score = torch.cat([pos_score, neg_score])\n",
    "        pos_label = torch.ones_like(pos_score)\n",
    "        neg_label = torch.zeros_like(neg_score)\n",
    "        labels = torch.cat([pos_label, neg_label])\n",
    "        loss = F.binary_cross_entropy_with_logits(score, labels)\n",
    "        self.train_loss(loss)\n",
    "        self.log(\n",
    "            \"train_loss\",\n",
    "            self.train_loss,\n",
    "            prog_bar=True,\n",
    "            on_step=True,\n",
    "            on_epoch=False,\n",
    "            batch_size=self.batch_size,\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_nodes, pos_graph, neg_graph, blocks = batch\n",
    "        x = blocks[0].srcdata[\"feat\"]\n",
    "        logits = self.module(blocks, x)\n",
    "        pos_score = self.predictor(pos_graph, logits)\n",
    "        neg_score = self.predictor(neg_graph, logits)\n",
    "\n",
    "        self.val_positive_distance(pos_score)\n",
    "        self.val_negative_distance(neg_score)\n",
    "\n",
    "        self.log(\n",
    "            \"mean_val_posititve_distance\",\n",
    "            self.val_positive_distance,\n",
    "            prog_bar=True,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            batch_size=self.batch_size,\n",
    "        )\n",
    "        self.log(\n",
    "            \"mean_val_negative_distance\",\n",
    "            self.val_negative_distance,\n",
    "            prog_bar=True,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            batch_size=self.batch_size,\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    device = \"cpu\"\n",
    "else:\n",
    "    device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading data from cached files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                  | Type           | Params\n",
      "---------------------------------------------------------\n",
      "0 | module                | SAGE           | 525 K \n",
      "1 | predictor             | ScorePredictor | 0     \n",
      "2 | train_loss            | MeanMetric     | 0     \n",
      "3 | val_positive_distance | MeanMetric     | 0     \n",
      "4 | val_negative_distance | MeanMetric     | 0     \n",
      "---------------------------------------------------------\n",
      "525 K     Trainable params\n",
      "0         Non-trainable params\n",
      "525 K     Total params\n",
      "2.100     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/conda/zillow_MMKG/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:488: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n",
      "/ext3/conda/zillow_MMKG/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:135: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-process data loading (when num_workers > 1), `__len__` could be inaccurate if each worker is not configured independently to avoid having duplicate data.\n",
      "  rank_zero_warn(\n",
      "/ext3/conda/zillow_MMKG/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/ext3/conda/zillow_MMKG/lib/python3.8/site-packages/lightning_lite/utilities/data.py:63: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-process data loading (when num_workers > 1), `__len__` could be inaccurate if each worker is not configured independently to avoid having duplicate data.\n",
      "  rank_zero_warn(\n",
      "/ext3/conda/zillow_MMKG/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 71/71 [00:01<00:00, 64.11it/s, loss=0.667, v_num=10, train_loss=0.664, mean_val_posititve_distance=0.660, mean_val_negative_distance=0.0376]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 71/71 [00:01<00:00, 64.01it/s, loss=0.667, v_num=10, train_loss=0.664, mean_val_posititve_distance=0.660, mean_val_negative_distance=0.0376]\n"
     ]
    }
   ],
   "source": [
    "modal_node_ids_file = '../'+cfg.data.zillow_root+'/modal_node_ids.json'\n",
    "datamodule = DataModule(\n",
    "    '../'+cfg.data.zillow_root, modal_node_ids_file, device=device, batch_size=cfg.training.batch_size\n",
    ")\n",
    "\n",
    "model = SAGELightning(\n",
    "    datamodule.in_dim,\n",
    "    cfg.model.hidden_dim,\n",
    "    n_layers=cfg.model.n_layers,\n",
    "    batch_size=cfg.training.batch_size,\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"mean_val_negative_distance\", save_top_k=1, mode=\"max\"\n",
    ")\n",
    "trainer = Trainer(accelerator=\"gpu\", max_epochs=10, callbacks=[checkpoint_callback])\n",
    "\n",
    "trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading data from cached files.\n"
     ]
    }
   ],
   "source": [
    "modal_node_ids_file = '../'+cfg.data.zillow_root+'/modal_node_ids.json'\n",
    "datamodule = DataModule(\n",
    "    '../'+cfg.data.zillow_root, modal_node_ids_file, device=device, batch_size=cfg.training.batch_size\n",
    ")\n",
    "val_dataloader = datamodule.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "383\n",
      "1355\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "end = 58\n",
    "max_img_id = max(modal_node_ids['images'])\n",
    "min_kw_id = min(modal_node_ids['keywords'])\n",
    "\n",
    "same_node_connect_pos_subgraph = []\n",
    "same_node_connect_neg_subgraph = []\n",
    "\n",
    "for batch in val_dataloader:\n",
    "    input_nodes, pos_graph, neg_graph, blocks = batch\n",
    "    pos_mapper = dict(zip(pos_graph.nodes().detach().cpu().numpy(), \n",
    "                pos_graph.ndata['_ID'].detach().cpu().numpy()))\n",
    "\n",
    "    neg_mapper = dict(zip(neg_graph.nodes().detach().cpu().numpy(), \n",
    "                neg_graph.ndata['_ID'].detach().cpu().numpy()))\n",
    "\n",
    "    src_pos, dst_pos = [x.detach().cpu().numpy() for x in pos_graph.edges()]\n",
    "    src_neg, dst_neg = [x.detach().cpu().numpy() for x in neg_graph.edges()]\n",
    "    \n",
    "    src_pos = [pos_mapper[src] for src in src_pos]\n",
    "    dst_pos = [pos_mapper[dst] for dst in dst_pos]\n",
    "\n",
    "    src_neg = [neg_mapper[src] for src in src_neg]\n",
    "    dst_neg = [neg_mapper[dst] for dst in dst_neg]\n",
    "\n",
    "    src_dst_pos, src_dst_neg = list(zip(src_pos, dst_pos)), list(zip(src_neg, dst_neg))\n",
    "\n",
    "    for pair in src_dst_pos:\n",
    "        if (pair[0]<max_img_id and pair[1]<max_img_id) or (pair[0]>min_kw_id and pair[1]>min_kw_id):\n",
    "            same_node_connect_pos_subgraph.append(pair)\n",
    "    \n",
    "    for pair in src_dst_neg:\n",
    "        if (pair[0]<max_img_id and pair[1]<max_img_id) or (pair[0]>min_kw_id and pair[1]>min_kw_id):\n",
    "            same_node_connect_neg_subgraph.append(pair)\n",
    "    \n",
    "    i += 1\n",
    "    if i == end:\n",
    "        break\n",
    "\n",
    "print(len(same_node_connect_pos_subgraph))\n",
    "print(len(same_node_connect_neg_subgraph))\n",
    "print(len(src_dst_pos))\n",
    "print(len(src_dst_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 256])\n"
     ]
    }
   ],
   "source": [
    "test_dataloader = datamodule.test_dataloader()\n",
    "new_old_node_id_mapping = json.load(open(f'../{cfg.data.zillow_root}/new_old_node_id_mapping.json', 'r'))\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    inputs, pos_graph, neg_graph, blocks = batch\n",
    "    pos_mapper = dict(zip(pos_graph.nodes().detach().cpu().numpy(), \n",
    "                pos_graph.ndata['_ID'].detach().cpu().numpy()))\n",
    "    \n",
    "    # update pos_graph node embeddings\n",
    "    blocks = [block.to(device) for block in blocks]\n",
    "    model = model.to(device)\n",
    "    x = blocks[0].srcdata[\"feat\"]\n",
    "    logits = model.module(blocks, x)\n",
    "    pos_graph.ndata[\"h\"] = logits\n",
    "\n",
    "    src_premap, dst_premap = [x.detach().cpu().numpy() for x in pos_graph.edges()]\n",
    "    src = [pos_mapper[x] for x in src_premap]\n",
    "    dst = [pos_mapper[x] for x in dst_premap]\n",
    "    nodes = [pos_mapper[x] for x in pos_graph.nodes().detach().cpu().tolist()]\n",
    "\n",
    "    sample_size = 10\n",
    "    sample_src = src[:sample_size]\n",
    "    sample_dst = dst[:sample_size]\n",
    "\n",
    "    src_node_indices = torch.LongTensor([nodes.index(node_id) for node_id in sample_src]).to(device)\n",
    "    dst_node_indices = torch.LongTensor([nodes.index(node_id) for node_id in sample_dst]).to(device)\n",
    "\n",
    "\n",
    "    # Embeddings of source and destination nodes in positive graph before and after GraphSAGE updates\n",
    "    sample_src_embeds_pre = pos_graph.ndata['feat'][src_node_indices]\n",
    "    sample_dst_embeds_pre = pos_graph.ndata['feat'][dst_node_indices]\n",
    "    sample_src_embeds_post = pos_graph.ndata['h'][src_node_indices]\n",
    "    sample_dst_embeds_post = pos_graph.ndata['h'][dst_node_indices]\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b8a98180768ec50653acfbae9679ecd2014a1d8366e4dd2cee9bea05835201d8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
