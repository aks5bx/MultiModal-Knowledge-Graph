{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import pyrootutils\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "import hydra\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics import MeanMetric\n",
    "from pytorch_lightning import LightningDataModule, LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torchmetrics.classification import BinaryAUROC, BinaryAveragePrecision\n",
    "from sklearn import metrics\n",
    "\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate having cfg available by loading in hydra config as dict\n",
    "import yaml\n",
    "try:\n",
    "    from yaml import CLoader as Loader, CDumper as Dumper\n",
    "except ImportError:\n",
    "    from yaml import Loader, Dumper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_net_id = os.getlogin()\n",
    "home_path = '/scratch/' + user_net_id + '/projects/NYU-Zillow-Capstone-2022-Team-A'\n",
    "if home_path not in sys.path:\n",
    "    sys.path.append('/scratch/' + user_net_id + '/projects/NYU-Zillow-Capstone-2022-Team-A')\n",
    "\n",
    "from src.datamodules.negative_sampler import NegativeSampler\n",
    "from src.model.SAGE import SAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Modules**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_bidirected_with_reverse_mapping(g):\n",
    "    \"\"\"Makes a graph bidirectional, and returns a mapping array ``mapping`` where ``mapping[i]``\n",
    "    is the reverse edge of edge ID ``i``. Does not work with graphs that have self-loops.\n",
    "    \"\"\"\n",
    "    g_simple, mapping = dgl.to_simple(\n",
    "        dgl.add_reverse_edges(g), return_counts=\"count\", writeback_mapping=True\n",
    "    )\n",
    "    c = g_simple.edata[\"count\"]\n",
    "    num_edges = g.num_edges()\n",
    "    mapping_offset = torch.zeros(g_simple.num_edges() + 1, dtype=g_simple.idtype)\n",
    "    mapping_offset[1:] = c.cumsum(0)\n",
    "    idx = mapping.argsort()\n",
    "    idx_uniq = idx[mapping_offset[:-1]]\n",
    "    reverse_idx = torch.where(\n",
    "        idx_uniq >= num_edges, idx_uniq - num_edges, idx_uniq + num_edges\n",
    "    )\n",
    "    reverse_mapping = mapping[reverse_idx]\n",
    "    # sanity check\n",
    "    src1, dst1 = g_simple.edges()\n",
    "    src2, dst2 = g_simple.find_edges(reverse_mapping)\n",
    "    assert torch.equal(src1, dst2)\n",
    "    assert torch.equal(src2, dst1)\n",
    "    return g_simple, reverse_mapping\n",
    "\n",
    "\n",
    "class NegativeSamplerTest(object):\n",
    "    def __init__(self, g, k, max_img_id, keyword_as_src, neg_share=False):\n",
    "        self.weights = g.in_degrees().float() ** 0.75\n",
    "        self.k = k\n",
    "        self.neg_share = neg_share\n",
    "        self.max_img_id = max_img_id\n",
    "        self.keyword_as_src = keyword_as_src\n",
    "\n",
    "    def __call__(self, g, eids):\n",
    "        src, _ = g.find_edges(eids)\n",
    "        if self.keyword_as_src == False:\n",
    "            img_node_mask = src <= self.max_img_id\n",
    "            src = src[img_node_mask]\n",
    "        n = len(src)\n",
    "\n",
    "        if self.neg_share and n % self.k == 0:\n",
    "            dst = self.weights.multinomial(n, replacement=True)\n",
    "            dst = dst.view(-1, 1, self.k).expand(-1, self.k, -1).flatten()\n",
    "        else:\n",
    "            dst = self.weights.multinomial(n * self.k, replacement=True)\n",
    "            \n",
    "        src = src.repeat_interleave(self.k)\n",
    "        return src, dst\n",
    "\n",
    "class DataModule(LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        csv_dataset_root,\n",
    "        modal_node_ids_file,\n",
    "        keyword_as_src=False,\n",
    "        data_cpu=False,\n",
    "        fan_out=[10, 25, 40],\n",
    "        device=\"cpu\",\n",
    "        batch_size=1024,\n",
    "        num_workers=4,\n",
    "        force_reload=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        dataset = dgl.data.CSVDataset(csv_dataset_root, force_reload=force_reload)\n",
    "        g = dataset[0]\n",
    "        g_bid, reverse_eids = to_bidirected_with_reverse_mapping(g)\n",
    "        g_bid = g_bid.to(device)\n",
    "        g = g.to(device)\n",
    "        reverse_eids = reverse_eids.to(device)\n",
    "\n",
    "        max_img_id = max(json.load(open(modal_node_ids_file, 'r'))['images'])\n",
    "\n",
    "        train_nid = torch.nonzero(g_bid.ndata[\"train_mask\"], as_tuple=True)[0].to(device)\n",
    "        val_nid = torch.nonzero(g_bid.ndata[\"val_mask\"], as_tuple=True)[0].to(device)\n",
    "        test_nid = torch.nonzero(\n",
    "            ~(g_bid.ndata[\"train_mask\"] | g_bid.ndata[\"val_mask\"]), as_tuple=True\n",
    "        )[0].to(device)\n",
    "\n",
    "        sampler = dgl.dataloading.MultiLayerNeighborSampler(\n",
    "            [int(_) for _ in fan_out], prefetch_node_feats=[\"feat\"]\n",
    "        )\n",
    "\n",
    "        self.g = g\n",
    "        self.g_bid = g_bid\n",
    "        self.train_nid, self.val_nid, self.test_nid = train_nid, val_nid, test_nid\n",
    "        self.sampler = sampler\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.in_dim = g_bid.ndata[\"feat\"].shape[1]\n",
    "        self.reverse_eids = reverse_eids\n",
    "        self.max_img_id = max_img_id\n",
    "        self.keyword_as_src = keyword_as_src\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        edge_sampler = dgl.dataloading.as_edge_prediction_sampler(\n",
    "            self.sampler,\n",
    "            exclude='reverse_id',\n",
    "            reverse_eids=self.reverse_eids,\n",
    "            negative_sampler=NegativeSamplerTest(self.g, 1, self.max_img_id, self.keyword_as_src)\n",
    "        )\n",
    "\n",
    "        train_subgraph = self.g_bid.subgraph(self.train_nid)\n",
    "        train_u, train_v = train_subgraph.edges()\n",
    "        train_eids = train_subgraph.edata['_ID'][train_subgraph.edge_ids(train_u, train_v)]\n",
    "\n",
    "        return dgl.dataloading.DataLoader(\n",
    "            self.g_bid,\n",
    "            train_eids,\n",
    "            edge_sampler,\n",
    "            device=self.device,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=False\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        edge_sampler = dgl.dataloading.as_edge_prediction_sampler(\n",
    "            self.sampler,\n",
    "        )\n",
    "\n",
    "        val_subgraph = self.g_bid.subgraph(self.val_nid)\n",
    "        val_u, val_v = val_subgraph.edges()\n",
    "        val_eids = val_subgraph.edata['_ID'][val_subgraph.edge_ids(val_u, val_v)]\n",
    "\n",
    "        return dgl.dataloading.DataLoader(\n",
    "            self.g_bid,\n",
    "            val_eids,\n",
    "            edge_sampler,\n",
    "            device=self.device,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            drop_last=False\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        edge_sampler = dgl.dataloading.as_edge_prediction_sampler(\n",
    "            self.sampler,\n",
    "        )\n",
    "\n",
    "        test_subgraph = self.g_bid.subgraph(self.test_nid)\n",
    "        test_u, test_v = test_subgraph.edges()\n",
    "        test_eids = test_subgraph.edata['_ID'][test_subgraph.edge_ids(test_u, test_v)]\n",
    "\n",
    "        return dgl.dataloading.DataLoader(\n",
    "            self.g_bid,\n",
    "            test_eids,\n",
    "            edge_sampler,\n",
    "            device=self.device,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScorePredictor(nn.Module):\n",
    "    def forward(self, edge_subgraph, x):\n",
    "        with edge_subgraph.local_scope():\n",
    "            edge_subgraph.ndata[\"h\"] = x\n",
    "            edge_subgraph.ndata['h_norm'] = F.normalize(x, p=2, dim=-1)\n",
    "            edge_subgraph.apply_edges(fn.u_dot_v(\"h_norm\", \"h_norm\", \"score\"))\n",
    "            return edge_subgraph.edata[\"score\"]\n",
    "\n",
    "class SAGELightning(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim,\n",
    "        h_dim,\n",
    "        n_layers=3,\n",
    "        activation=F.relu,\n",
    "        dropout=0,\n",
    "        sage_conv_method=\"mean\",\n",
    "        lr=0.0005,\n",
    "        batch_size=1024,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.module = SAGE(\n",
    "            in_dim, h_dim, n_layers, activation, dropout, sage_conv_method\n",
    "        )\n",
    "        self.lr = lr\n",
    "        self.predictor = ScorePredictor()\n",
    "        self.batch_size = batch_size\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.train_loss = MeanMetric()\n",
    "        self.mean_val_positive_score = MeanMetric()\n",
    "    \n",
    "    def forward(self, graph, blocks, x):\n",
    "        self.module(graph, blocks, x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_nodes, pos_graph, neg_graph, blocks = batch\n",
    "        x = blocks[0].srcdata[\"feat\"]\n",
    "        logits = self.module(blocks, x)\n",
    "        pos_score = self.predictor(pos_graph, logits)\n",
    "        neg_score = self.predictor(neg_graph, logits)\n",
    "\n",
    "        score = torch.cat([pos_score, neg_score])\n",
    "        pos_label = torch.ones_like(pos_score)\n",
    "        neg_label = torch.zeros_like(neg_score)\n",
    "        labels = torch.cat([pos_label, neg_label])\n",
    "        loss = F.binary_cross_entropy_with_logits(score, labels)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_nodes, pos_graph, blocks = batch\n",
    "        x = blocks[0].srcdata[\"feat\"]\n",
    "        logits = self.module(blocks, x)\n",
    "        pos_score = self.predictor(pos_graph, logits)\n",
    "        pos_label = torch.ones_like(pos_score)\n",
    "        self.mean_val_positive_score(pos_score)\n",
    "\n",
    "        self.log(\n",
    "            \"mean_val_positive_score\",\n",
    "            self.mean_val_positive_score,\n",
    "            prog_bar=True,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            batch_size=self.batch_size,\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Notebook Run - Preperation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directory Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NestedNamespace(SimpleNamespace):\n",
    "    def __init__(self, dictionary, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        for key, value in dictionary.items():\n",
    "            if isinstance(value, dict):\n",
    "                self.__setattr__(key, NestedNamespace(value))\n",
    "            else:\n",
    "                self.__setattr__(key, value)\n",
    "\n",
    "root_path = pyrootutils.find_root(search_from='train_graphsage_explore.ipynb', indicator=\".git\")\n",
    "print('Set WD location to', root_path)\n",
    "pyrootutils.set_root(\n",
    "    path=root_path,\n",
    "    project_root_env_var=True,\n",
    "    dotenv=True,\n",
    "    pythonpath=True,\n",
    "    cwd=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if not torch.cuda.is_available():\n",
    "        device = \"cpu\"\n",
    "        print('No GPU available, using CPU')\n",
    "    else:\n",
    "        device = \"cuda\"\n",
    "        print('Using GPU')\n",
    "except:\n",
    "    device = \"cpu\"\n",
    "    print('No GPU available, using CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Notebook Run**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla Graph Training\n",
    "cfg = NestedNamespace(yaml.load(open('conf/config.yaml'), Loader=Loader))\n",
    "org = 'zillow'\n",
    "connect_type = '_images_975'\n",
    "\n",
    "if org == 'coco':\n",
    "    csv_dataset_root = cfg.data.coco_graph_root\n",
    "elif org == 'zillow':\n",
    "    csv_dataset_root = cfg.data.zillow_root + connect_type\n",
    "\n",
    "modal_node_ids_file = os.path.join(csv_dataset_root,'modal_node_ids.json')\n",
    "datamodule = DataModule(\n",
    "    csv_dataset_root, \n",
    "    modal_node_ids_file, \n",
    "    keyword_as_src=False, \n",
    "    device=device, \n",
    "    batch_size=cfg.training.batch_size, \n",
    "    force_reload=False\n",
    ")\n",
    "\n",
    "model = SAGELightning(\n",
    "    datamodule.in_dim,\n",
    "    cfg.model.hidden_dim,\n",
    "    n_layers=cfg.model.n_layers,\n",
    "    batch_size=cfg.training.batch_size,\n",
    "    lr=cfg.training.learning_rate\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"mean_val_positive_score\", save_top_k=1, mode=\"max\"\n",
    ")\n",
    "trainer = Trainer(accelerator=\"gpu\", max_epochs=cfg.training.n_epochs, callbacks=[checkpoint_callback])\n",
    "trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconnecting Train and Val Subgraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconnecting Via Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialize Val and Eval Subgraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize val and eval subgraph\n",
    "val_subgraph = datamodule.g_bid.subgraph(datamodule.val_nid)\n",
    "eval_subgraph = datamodule.g_bid.subgraph(datamodule.train_nid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combine Nodes Only (No Edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_img_node_idxs = (val_subgraph.ndata['ntype'] == 0).nonzero().squeeze()\n",
    "val_img_embeds = val_subgraph.ndata['feat'][val_img_node_idxs]\n",
    "val_img_node_ids = val_subgraph.ndata['_ID'][val_img_node_idxs]\n",
    "\n",
    "print('number of val img nodes:', len(val_img_node_ids))\n",
    "\n",
    "val_nodes_data = {'train_mask': torch.zeros(len(val_img_node_ids), dtype=torch.uint8).to(device),\n",
    "                  'val_mask': torch.ones(len(val_img_node_ids), dtype=torch.uint8).to(device),\n",
    "                  'test_mask': torch.zeros(len(val_img_node_ids), dtype=torch.uint8).to(device),\n",
    "                  'ntype': torch.zeros(len(val_img_node_ids), dtype=torch.int64).to(device),\n",
    "                  'feat': val_img_embeds.to(device),\n",
    "                  '_ID': val_img_node_ids}\n",
    "\n",
    "eval_subgraph.add_nodes(num=len(val_img_node_ids), data=val_nodes_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Identify Image Node Pairs as Potential Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Identify image node pairs as edges\n",
    "\n",
    "eval_train_img_node_idxs = ((eval_subgraph.ndata['ntype'] == 0)&(eval_subgraph.ndata['train_mask']==1)).nonzero().squeeze()\n",
    "eval_val_img_node_idxs = ((eval_subgraph.ndata['ntype'] == 0)&(eval_subgraph.ndata['val_mask']==1)).nonzero().squeeze()\n",
    "eval_train_img_embeds = eval_subgraph.ndata['feat'][eval_train_img_node_idxs]\n",
    "eval_val_img_embeds = eval_subgraph.ndata['feat'][eval_val_img_node_idxs]\n",
    "\n",
    "cosine_sims_matrix = metrics.pairwise.cosine_similarity(eval_val_img_embeds.cpu().detach().numpy(), \n",
    "                                                        eval_train_img_embeds.cpu().detach().numpy())\n",
    "sim_threshold = 0.98\n",
    "        \n",
    "image_matches = []\n",
    "for cosine_sims in tqdm(cosine_sims_matrix, desc='computing image matches'):\n",
    "    eval_train_node_id_matches = eval_train_img_node_idxs[(cosine_sims>sim_threshold)]\n",
    "    if len(eval_train_node_id_matches) == 0:\n",
    "        eval_train_node_id_matches = eval_train_img_node_idxs[np.argmax(cosine_sims)].unsqueeze(0)\n",
    "    image_matches.append(eval_train_node_id_matches.tolist())\n",
    "\n",
    "matches_per_img = [len(matches) for matches in image_matches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('match stats:')\n",
    "print(f'min matches: {np.min(matches_per_img)}')\n",
    "print(f'max matches: {np.max(matches_per_img)}')\n",
    "print(f'avg matches: {np.mean(matches_per_img)}')\n",
    "print(f'std matches: {np.std(matches_per_img)}')\n",
    "print(f'total new edges to add: {sum(matches_per_img)*2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add Edges to Eval Subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Add the edges to eval_subgraph\n",
    "\n",
    "u = []\n",
    "v = []\n",
    "\n",
    "for i in range(len(image_matches)):\n",
    "    val_img_node = eval_val_img_node_idxs[i].item()\n",
    "    train_img_matches = image_matches[i]\n",
    "    for node_id in train_img_matches:\n",
    "        train_img_node = node_id\n",
    "        # Add bidirectional edge for each match\n",
    "        u += [val_img_node, train_img_node]\n",
    "        v += [train_img_node, val_img_node]\n",
    "\n",
    "edge_data = {'_ID': torch.arange(torch.max(eval_subgraph.edata['_ID'])+1, torch.max(eval_subgraph.edata['_ID'])+1+len(u), dtype=torch.int64).to(device)}\n",
    "eval_subgraph.add_edges(torch.LongTensor(u).to(device), torch.LongTensor(v).to(device), data=edge_data)\n",
    "eval_subgraph = eval_subgraph.add_self_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert DGL Graph into GraphSAGE Compatible Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Turn DGL graph into DataLoader object for GraphSAGE forward inference\n",
    "\n",
    "u_eval, v_eval = eval_subgraph.edges()\n",
    "eval_subgraph_eids = eval_subgraph.edge_ids(u_eval, v_eval)\n",
    "layer_sampler = dgl.dataloading.NeighborSampler(fanouts=[10, 25]) # During message passing between GNN layers, each node accept messages from a maximum of 25 incoming nodes\n",
    "batch_size = len(eval_subgraph.edges()[0])\n",
    "\n",
    "def eval_dataloader(g, layer_sampler, batch_size, eids):\n",
    "    edge_sampler = dgl.dataloading.as_edge_prediction_sampler(layer_sampler)\n",
    "\n",
    "    return dgl.dataloading.DataLoader(\n",
    "        g,\n",
    "        eids,\n",
    "        edge_sampler,\n",
    "        device=device,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False\n",
    "        # num_workers=self.num_workers,\n",
    "    )\n",
    "\n",
    "eval_dl = eval_dataloader(eval_subgraph, layer_sampler, batch_size, eval_subgraph_eids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run GraphSAGE Inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Run graphSAGE forward inference over entire val_subgraph message flow graph (MFG)\n",
    "\n",
    "for batch in eval_dl:\n",
    "    # This loop only runs once b/c batch_size = number of total edges in train_val_subgraph - we only need it to get \"blocks\"\n",
    "    inputs, edge_subgraph, blocks = batch\n",
    "    \n",
    "x = blocks[0].srcdata[\"feat\"]\n",
    "model = model.to(device)\n",
    "logits = model.module(blocks, x)\n",
    "\n",
    "eval_subgraph.ndata['feat_pred_norm'] = F.normalize(logits, p=2, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract Validation Image Features and Keyword Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Extract validation image features and keyword features for \n",
    "eval_val_img_node_ids = ((eval_subgraph.ndata['val_mask']==1)&(eval_subgraph.ndata['ntype']==0)).nonzero().squeeze()\n",
    "eval_keyword_node_ids = ((eval_subgraph.ndata['ntype']==1)).nonzero().squeeze()\n",
    "\n",
    "# Post-GraphSAGE embeddings\n",
    "eval_val_img_feat_sage = eval_subgraph.ndata['feat_pred_norm'][eval_val_img_node_ids]\n",
    "eval_keyword_feat_sage = eval_subgraph.ndata['feat_pred_norm'][eval_keyword_node_ids]\n",
    "\n",
    "# Original CLIP embeddings before GraphSAGE forward method\n",
    "eval_subgraph.ndata['feat_norm'] = F.normalize(eval_subgraph.ndata['feat'], p=2, dim=-1)\n",
    "eval_val_img_feat_clip = eval_subgraph.ndata['feat_norm'][eval_val_img_node_ids]\n",
    "eval_keyword_feat_clip = eval_subgraph.ndata['feat_norm'][eval_keyword_node_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute Cosine Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = False\n",
    "\n",
    "# Step 8: Compute cosine similarities between validation images and keywords to get \"link scores\" between 0 and 1\n",
    "\n",
    "val_sage_link_scores = metrics.pairwise.cosine_similarity(eval_keyword_feat_sage.cpu().detach().numpy(),\n",
    "                                                          eval_val_img_feat_sage.cpu().detach().numpy())\n",
    "val_clip_link_scores = metrics.pairwise.cosine_similarity(eval_keyword_feat_clip.cpu().detach().numpy(),\n",
    "                                                          eval_val_img_feat_clip.cpu().detach().numpy())\n",
    "\n",
    "if verbose:\n",
    "    print('means:')\n",
    "    print('sage:', np.mean(val_sage_link_scores))\n",
    "    print('clip:', np.mean(val_clip_link_scores))\n",
    "    print('mins:')\n",
    "    print('sage:', np.min(val_sage_link_scores))\n",
    "    print('clip:', np.min(val_clip_link_scores))\n",
    "    print('maxs:')\n",
    "    print('sage:', np.max(val_sage_link_scores))\n",
    "    print('clip:', np.max(val_clip_link_scores))\n",
    "    print('stds:')\n",
    "    print('sage:', np.std(val_sage_link_scores))\n",
    "    print('clip:', np.std(val_clip_link_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predict and Compute Precision/Recall/Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Get true labels for each keyword from validation subgraph adjacency matrix\n",
    "# Adjacency matrix needs to be sub-setted such that rows correspond only to image nodes and columns correspond only to keyword nodes\n",
    "val_img_indices = (val_subgraph.ndata['ntype']==0).nonzero().cpu().reshape(1, -1)\n",
    "val_keyword_indices = (val_subgraph.ndata['ntype']==1).nonzero().cpu().reshape(-1, 1)\n",
    "\n",
    "val_adj_matrix = val_subgraph.adjacency_matrix().to_dense().numpy()\n",
    "val_adj_matrix = val_adj_matrix[val_keyword_indices, val_img_indices]\n",
    "\n",
    "\n",
    "# Step 10: Make predictions based on prediction threshold and get precision, recall, and accuracy \n",
    "pred_thresholds = np.linspace(0.1, 0.5, 30)\n",
    "sage_clip_metrics = pd.DataFrame()\n",
    "\n",
    "for pred_threshold in pred_thresholds:\n",
    "    val_sage_link_predictions = (val_sage_link_scores > pred_threshold).astype(int)\n",
    "    val_clip_link_predictions = (val_clip_link_scores > pred_threshold).astype(int)\n",
    "\n",
    "    results_dict = {'sage': {'tp': np.empty(len(val_sage_link_predictions)),\n",
    "                             'fp': np.empty(len(val_sage_link_predictions)),\n",
    "                             'fn': np.empty(len(val_sage_link_predictions)),\n",
    "                             'actual_p': np.empty(len(val_sage_link_predictions)),\n",
    "                             'precision': np.empty(len(val_sage_link_predictions)),\n",
    "                             'recall': np.empty(len(val_sage_link_predictions))},\n",
    "                    'clip': {'tp': np.empty(len(val_sage_link_predictions)),\n",
    "                             'fp': np.empty(len(val_sage_link_predictions)),\n",
    "                             'fn': np.empty(len(val_sage_link_predictions)),\n",
    "                             'actual_p': np.empty(len(val_sage_link_predictions)),\n",
    "                             'precision': np.empty(len(val_sage_link_predictions)),\n",
    "                             'recall': np.empty(len(val_sage_link_predictions))}}\n",
    "\n",
    "    weights = np.empty(len(val_sage_link_predictions))\n",
    "\n",
    "    for i in range(len(val_sage_link_predictions)):\n",
    "        sage_tp = np.sum(((val_sage_link_predictions[i]==1)&(val_adj_matrix[i]==1)))\n",
    "        sage_fp = np.sum(((val_sage_link_predictions[i]==1)&(val_adj_matrix[i]==0)))\n",
    "        sage_fn = np.sum(((val_sage_link_predictions[i]==0)&(val_adj_matrix[i]==1)))\n",
    "        sage_p = np.sum(val_sage_link_predictions[i])\n",
    "        \n",
    "        clip_tp = np.sum(((val_clip_link_predictions[i]==1)&(val_adj_matrix[i]==1)))\n",
    "        clip_fp = np.sum(((val_clip_link_predictions[i]==1)&(val_adj_matrix[i]==0)))\n",
    "        clip_fn = np.sum(((val_clip_link_predictions[i]==0)&(val_adj_matrix[i]==1)))\n",
    "        clip_p = np.sum(val_clip_link_predictions[i])\n",
    "\n",
    "        true_p = np.sum(val_adj_matrix[i])\n",
    "        \n",
    "        results_dict['sage']['tp'][i] = sage_tp\n",
    "        results_dict['sage']['fp'][i] = sage_fp\n",
    "        results_dict['sage']['fn'][i] = sage_fn\n",
    "        results_dict['sage']['actual_p'][i] = true_p\n",
    "        results_dict['sage']['precision'][i] = sage_tp / sage_p if sage_p > 0 else 0\n",
    "        results_dict['sage']['recall'][i] = sage_tp / true_p if true_p > 0 else 0\n",
    "\n",
    "        results_dict['clip']['tp'][i] = clip_tp\n",
    "        results_dict['clip']['fp'][i] = clip_fp\n",
    "        results_dict['clip']['fn'][i] = clip_fn\n",
    "        results_dict['clip']['actual_p'][i] = true_p\n",
    "        results_dict['clip']['precision'][i] = clip_tp / clip_p if clip_p > 0 else 0\n",
    "        results_dict['clip']['recall'][i] = clip_tp / true_p if true_p > 0 else 0\n",
    "\n",
    "        weights[i] = true_p\n",
    "\n",
    "    weights /= np.sum(weights)\n",
    "\n",
    "    for method in results_dict.keys():\n",
    "        row = {'threshold': pred_threshold, 'method': method}\n",
    "        for metric in results_dict[method]:\n",
    "            if metric == 'precision' or metric == 'recall':\n",
    "                row[f'{metric}_micro'] = np.mean(results_dict[method][metric]*weights)\n",
    "                row[f'{metric}_macro'] = np.mean(results_dict[method][metric])\n",
    "            else:\n",
    "                row[metric] = np.mean(results_dict[method][metric])\n",
    "        sage_clip_metrics = pd.concat([sage_clip_metrics, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "sage_metrics = sage_clip_metrics[(sage_clip_metrics['method']=='sage')]\n",
    "clip_metrics = sage_clip_metrics[(sage_clip_metrics['method']=='clip')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best SAGE metrics: ')\n",
    "print('Precision, Recall at Max Recall:\\n', sage_metrics[sage_metrics['recall_macro']==sage_metrics['recall_macro'].max()][['threshold', 'precision_macro', 'recall_macro']].iloc[0,:])\n",
    "print('Precision, Recall at Max Precision:\\n', sage_metrics[sage_metrics['precision_macro']==sage_metrics['precision_macro'].max()][['threshold', 'precision_macro', 'recall_macro']].iloc[0,:])\n",
    "\n",
    "print('Best CLIP metrics: ')\n",
    "print('Precision, Recall at Max Recall:\\n', clip_metrics[clip_metrics['recall_macro']==clip_metrics['recall_macro'].max()][['threshold', 'precision_macro', 'recall_macro']].iloc[0,:])\n",
    "print('Precision, Recall at Max Precision:\\n', clip_metrics[clip_metrics['precision_macro']==clip_metrics['precision_macro'].max()][['threshold', 'precision_macro', 'recall_macro']].iloc[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sage_micro_threshold_metrics = sage_clip_metrics[sage_clip_metrics['method']=='sage'][['threshold', 'precision_micro', 'recall_micro']]\n",
    "clip_micro_threshold_metrics = sage_clip_metrics[sage_clip_metrics['method']=='clip'][['threshold', 'precision_micro', 'recall_micro']]\n",
    "sage_macro_threshold_metrics = sage_clip_metrics[sage_clip_metrics['method']=='sage'][['threshold', 'precision_macro', 'recall_macro']]\n",
    "clip_macro_threshold_metrics = sage_clip_metrics[sage_clip_metrics['method']=='sage'][['threshold', 'precision_macro', 'recall_macro']]\n",
    "\n",
    "\n",
    "for avg_type in ['micro', 'macro']:\n",
    "    legend = []\n",
    "    for metric in ['precision', 'recall']:\n",
    "        for method in ['sage', 'clip']:\n",
    "            plt.plot(sage_clip_metrics[sage_clip_metrics['method']==method]['threshold'], \n",
    "                    sage_clip_metrics[sage_clip_metrics['method']==method][f'{metric}_{avg_type}'])\n",
    "            legend += [f'{method}_{metric}']\n",
    "        plt.legend(legend)\n",
    "        plt.xlabel('positive prediction threshold')\n",
    "        plt.ylabel('metric')\n",
    "        plt.title(f'{avg_type}-averaged link prediction {metric}')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Additional Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side quest: check out how nodes with only self loops change in terms of their embeddings after running graphSAGE forward\n",
    "\n",
    "eval_subgraph_keyword_nodes = (eval_subgraph.ndata['ntype']==1).nonzero().squeeze()\n",
    "u, v = eval_subgraph.edges()\n",
    "uv = torch.cat((u,v))\n",
    "\n",
    "only_self_loops = []\n",
    "for node in tqdm(eval_subgraph_keyword_nodes):\n",
    "    count = torch.sum((uv==node))\n",
    "    if count == 2:\n",
    "        only_self_loops.append(node.item())\n",
    "\n",
    "print(eval_subgraph.ndata['feat_norm'][only_self_loops[0]][:10])\n",
    "print(eval_subgraph.ndata['feat_pred_norm'][only_self_loops[0]][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b8a98180768ec50653acfbae9679ecd2014a1d8366e4dd2cee9bea05835201d8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
